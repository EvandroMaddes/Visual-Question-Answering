# -*- coding: utf-8 -*-
"""FINALHomework3V0.ipynb

Automatically generated by Colaboratory.

##This notebook contains two model architectures: a custom model and a model based on the coattention mechanism.
"""

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

import os

import tensorflow as tf
import numpy as np

# Set the seed for random operations. 
SEED = 1234
tf.random.set_seed(SEED)

from google.colab import drive
drive.mount('/content/drive')

cwd = os.getcwd()

"""##Pre-processing of the data."""

!unzip '/content/drive/My Drive/Homeworks/Homework3/Datasets/anndl-2020-vqa'

#Answers dictionary as stated in the challenge
labels_dict = {
        '0': 0,
        '1': 1,
        '2': 2,
        '3': 3,
        '4': 4,
        '5': 5,
        'apple': 6,
        'baseball': 7,
        'bench': 8,
        'bike': 9,
        'bird': 10,
        'black': 11,
        'blanket': 12,
        'blue': 13,
        'bone': 14,
        'book': 15,
        'boy': 16,
        'brown': 17,
        'cat': 18,
        'chair': 19,
        'couch': 20,
        'dog': 21,
        'floor': 22,
        'food': 23,
        'football': 24,
        'girl': 25,
        'grass': 26,
        'gray': 27,
        'green': 28,
        'left': 29,
        'log': 30,
        'man': 31,
        'monkey bars': 32,
        'no': 33,
        'nothing': 34,
        'orange': 35,
        'pie': 36,
        'plant': 37,
        'playing': 38,
        'red': 39,
        'right': 40,
        'rug': 41,
        'sandbox': 42,
        'sitting': 43,
        'sleeping': 44,
        'soccer': 45,
        'squirrel': 46,
        'standing': 47,
        'stool': 48,
        'sunny': 49,
        'table': 50,
        'tree': 51,
        'watermelon': 52,
        'white': 53,
        'wine': 54,
        'woman': 55,
        'yellow': 56,
        'yes': 57
}

#Create the training dataframe
import json
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

MAX_NUM_WORDS = 20000

with open("/content/VQA_Dataset/train_questions_annotations.json") as read_file:
  train_dic = json.load(read_file)
train_dataframe = pd.DataFrame(train_dic.items())
train_dataframe.rename(columns = {0:'train_id', 1:'info'}, inplace = True)
L = [x for x in train_dataframe.pop('info').values]
temp_dataframe = pd.DataFrame(L)
#Create a complete dataframe with shape {train_id, question, image_id, answer}
train_dataframe = train_dataframe.join(temp_dataframe)
#replaces all answers with their correspondent labels encoded as int, they will be used for classification training
train_dataframe = train_dataframe.replace({"answer": labels_dict})


#encode questions with keras Tokenizer
train_sentences = train_dataframe.get('question')
train_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
train_tokenizer.fit_on_texts(train_sentences)
train_tokenized = train_tokenizer.texts_to_sequences(train_sentences)

#obtain the questions dictionary
train_wtoi = train_tokenizer.word_index

#retrieve the question with max lenght
max_train_lenght = max(len(train_sentence) for train_sentence in train_tokenized)
#pad all the questions in order to be of same lenght as the max
train_encoder_inputs = pad_sequences(train_tokenized, maxlen = max_train_lenght, padding='post')

train_dataframe.drop('question', axis=1, inplace=True)
train_dataframe['question'] = train_encoder_inputs.tolist()

train_dataframe

"""## Create Dataset objects."""

#Custom data generator
from PIL import Image
class DataGenerator(tf.keras.utils.Sequence):
    """Generates data for Keras."""
    def __init__(self, img_files, questions, labels, img_dir, which_subset='training',
                 preprocessing_function=None, img_data_generator = None,
                 batch_size=32, dim=[300, 300], n_channels=3,
                 n_classes=2):
        """Initialization.
        
        Args:
            img_files: A list of to image files.
            questions: A list of encoded questions.
            labels: A dictionary of corresponding labels.
        """

        self.img_files = img_files
        self.questions = questions
        self.labels = labels
        self.img_dir = img_dir
        self.which_subset = which_subset
        self.preprocessing_function = preprocessing_function
        self.img_data_generator = img_data_generator
        self.batch_size = batch_size
        self.dim = dim
        self.n_channels = n_channels
        self.n_classes = n_classes

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, index):
        # Generate data
        if self.which_subset == 'test':
          X,y = self.__data_generation(self.img_files[index], self.questions[index], 1)
        else:
          X, y = self.__data_generation(self.img_files[index], self.questions[index], self.labels[index])
        return X, y

    #Generates data sample.
    def __data_generation(self, img_files_temp, questions_temp, labels_temp):
        # Read image
        img_file = os.path.join(self.img_dir, img_files_temp)
        img_file = img_file + '.png'            
        img = Image.open(img_file)
        # Resize image
        img = img.resize(self.dim)
        img = np.array(img)
        img = img[...,:3]
        # convert from integers to floats
        img = img.astype('float32')
        # normalize to the range 0-1
        img /= 255.0
        #Apply, if requested, data augmentation
        if self.img_data_generator is not None:
          img_t = self.img_data_generator.get_random_transform(img.shape)
          img = self.img_data_generator.apply_transform(img, img_t)
        # preprocess the image, if requested
        if self.preprocessing_function is not None:
          img = self.preprocessing_function(img)

        X = (np.array(img), np.array(questions_temp))
        
        return X, tf.keras.utils.to_categorical(labels_temp, num_classes=self.n_classes)
        #return np.array(X_img), np.array(X_questions), tf.keras.utils.to_categorical(y, num_classes=self.n_classes)

# ImageDataGenerator
# ------------------

from tensorflow.keras.preprocessing.image import ImageDataGenerator

apply_data_augmentation = False
img_data_gen = None
if apply_data_augmentation:
    img_data_gen = ImageDataGenerator(rotation_range=0.4,
                                      width_shift_range=0.4,
                                      height_shift_range=0.4,
                                      zoom_range=0.3,
                                      horizontal_flip=True,
                                      vertical_flip=False,
                                      fill_mode='reflect')

#Create Training and Validation generator

#from tensorflow.keras.applications.vgg16 import preprocess_input 
from tensorflow.keras.applications.vgg19 import preprocess_input 
from sklearn import model_selection as ms
train_img_dir = os.path.join(cwd, 'VQA_Dataset/Images')
img_h = 200
img_w = 350
batch_size = 128
num_classes = len(labels_dict)
#Apply backbone network's preprocessing function
preprocessing = preprocess_input
#Shuffling the order
train_dataframe = train_dataframe.sample(frac=1, random_state=SEED).reset_index(drop=True)
#Split the whole dataframe in training (80%) and validation (20%) 
train_df, valid_df = ms.train_test_split(train_dataframe, test_size=0.2, stratify=train_dataframe['answer'],random_state=SEED)

train_gen = DataGenerator(img_files=train_df.get('image_id').to_list(),
                          questions=train_df.get('question').to_list(),
                          labels=train_df.get('answer').to_list(),
                          img_dir= train_img_dir,
                          n_classes=num_classes,
                          img_data_generator=img_data_gen,
                          preprocessing_function= preprocessing,
                          dim = [img_w,img_h])
valid_gen = DataGenerator(img_files=valid_df.get('image_id').to_list(),
                          questions=valid_df.get('question').to_list(),
                          labels=valid_df.get('answer').to_list(),
                          img_dir= train_img_dir,
                          n_classes=num_classes,
                          img_data_generator=img_data_gen,
                          preprocessing_function= preprocessing,
                          dim = [img_w,img_h])

#Create the datasets from generators
train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,
                                               output_types=((tf.float32, tf.int32), tf.int32),
                                               output_shapes=(([ img_h, img_w, 3],[ max_train_lenght]),(num_classes)))

train_dataset = train_dataset.batch(batch_size)

train_dataset = train_dataset.repeat()

valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen,
                                               output_types=((tf.float32, tf.int32), tf.int32),
                                               output_shapes=(([ img_h, img_w, 3],[ max_train_lenght]),(num_classes)))
valid_dataset = valid_dataset.batch(batch_size)

valid_dataset = valid_dataset.repeat()

#Fpr visualization, plot a histogram with occurence of each answer in training set
import matplotlib.pyplot as plt

heights = np.zeros(len(labels_dict))
for answer in train_dataframe.answer:
  heights[answer] += 1

fig = plt.gcf()
fig.set_size_inches(18.5, 10.5)
x = np.arange(len(labels_dict))
plt.bar(x, height=heights)
plt.xticks(x, list(range(0, len(labels_dict)+1)))

# Commented out IPython magic to ensure Python compatibility.
#For visualization purpose, retrieve a data sample from the dataset
import matplotlib.pyplot as plt
# %matplotlib inline

iterator = iter(train_dataset)

# Handling function that invert the tokenizer encoding into text
def textFromSequence(sequence, dictionary):
  key_list = list(dictionary.keys())
  value_list = list(dictionary.values())
  decoded = []
  for encoded_word in sequence:
    if encoded_word == 0:
      continue
    for word, value in dictionary.items():
      if value == encoded_word:
        decoded.append(word)
        continue
  return decoded

#Plot image, decoded question and correspondent answer
X, target = next(iterator)
augmented_img = X[0]
augmented_img = np.array(augmented_img[0])
question = X[1]
print(textFromSequence(question[0],train_wtoi))
print(target[0])
augmented_img = augmented_img * 255
plt.imshow(np.uint8(augmented_img))
plt.plot

"""## Architectures."""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Input
from tensorflow.keras.layers import Embedding, LSTM
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Model
def buildCustomModel(img_h, img_w, vocab_size, num_classes, ques_lenght = None,emb_dim=256):

  vision_model = Sequential()
  #vision_model.add(tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3)))
  vision_model.add(tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=(img_h, img_w, 3)))
  vision_model.add(Flatten())

  for layer in vision_model.layers:
    layer.trainable = False

  image_input = Input(shape=(img_h, img_w, 3))
  encoded_image = vision_model(image_input)

  question_input = Input(shape=(21, ) , dtype='int32')
  embedded_question = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=ques_lenght)(question_input) #NON HO IDEA DI COSA SIA INPUT LENGHT, OUT DIM E INPUT DIM
  encoded_question = tf.keras.layers.Bidirectional(LSTM(1024))(embedded_question)

  # Combine CNN and RNN to create the final model

  merged = tf.keras.layers.concatenate([ encoded_image, encoded_question])
  output = Dense(1024, activation='relu')(merged)
  output = Dropout(0.09)(output)
  output = Dense(512, activation='relu')(output)
  output = Dropout(0.05)(output)
  output = Dense(256, activation='relu')(output)
  output = Dense(num_classes, activation='softmax')(output)

  return Model(inputs=[image_input, question_input], outputs=output)

#Build and compile the CustomModel
l_rate = 1e-3
loss = tf.keras.losses.CategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam(learning_rate=l_rate)
metrics = ['accuracy']
model = buildCustomModel(img_h = img_h, img_w = img_w, 
                         vocab_size = len(train_wtoi)+1,
                         num_classes= num_classes,
                         ques_lenght=max_train_lenght)

model.compile(optimizer=optimizer, loss=loss, metrics= metrics)

#Coattention model class
class CoattentionModel(tf.keras.layers.Layer):
    def __init__(self, num_classes, num_embeddings):#, num_embeddings, num_classes, embed_dim=512, k=30
        super().__init__()
        self.num_classes = num_classes
        self.hidden_size = 512
        self.dropout = 0.3
        self.num_embeddings = num_embeddings

        self.image_dense = tf.keras.layers.Dense(self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_normal(seed=15)) 
        self.image_corr = tf.keras.layers.Dense(self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_normal(seed=29))

        self.image_atten_dense = tf.keras.layers.Dense(self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=17)) 
        self.question_atten_dens = tf.keras.layers.Dense(self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=28))
        self.question_atten_dropout = tf.keras.layers.Dropout(self.dropout)
        self.image_atten_dropout = tf.keras.layers.Dropout(self.dropout)

        self.ques_atten = tf.keras.layers.Dense(1, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=21))

        self.img_atten = tf.keras.layers.Dense(1, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=33))

        self.embed = tf.keras.layers.Embedding(self.num_embeddings, self.hidden_size,
                                               embeddings_initializer = tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23))
        
        self.unigram_conv = tf.keras.layers.Conv1D(filters = self.hidden_size, kernel_size = 1, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed=41))
        self.bigram_conv  = tf.keras.layers.Conv1D(filters = self.hidden_size, kernel_size = 2, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed=58), dilation_rate = 2)
        self.trigram_conv = tf.keras.layers.Conv1D(filters = self.hidden_size, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed=89), dilation_rate = 2)
        self.max_pool = tf.keras.layers.MaxPool2D((3,1))
        self.phrase_dropout = tf.keras.layers.Dropout(self.dropout)
        
        self.lstm = tf.keras.layers.LSTM(units = 512 , return_sequences=True, dropout = self.dropout,
                                         kernel_initializer = tf.keras.initializers.glorot_uniform(seed=26),
                                         recurrent_initializer = tf.keras.initializers.orthogonal(seed=54))
        
        self.tanh = tf.keras.layers.Activation('tanh')
        self.softmax = tf.keras.layers.Activation('softmax')
        
        self.W_w_dropout = tf.keras.layers.Dropout(self.dropout)
        self.W_p_dropout = tf.keras.layers.Dropout(self.dropout)
        self.W_s_dropout = tf.keras.layers.Dropout(self.dropout)

        self.W_w = tf.keras.layers.Dense(units = self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=32), input_shape = (self.hidden_size,))
        self.W_p = tf.keras.layers.Dense(units = self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=49), input_shape = (2 * self.hidden_size, ))
        self.W_s = tf.keras.layers.Dense(units = self.hidden_size, kernel_initializer = tf.keras.initializers.glorot_uniform(seed=31), input_shape = (2 * self.hidden_size, ))
        
        self.fc1_Dense = tf.keras.layers.Dense(units = 2 * self.hidden_size, activation='relu',
                                               kernel_initializer = tf.keras.initializers.he_normal(seed=84))
        self.fc1_dropout = tf.keras.layers.Dropout(self.dropout)

        self.fc = tf.keras.layers.Dense(units = self.num_classes, activation='softmax',kernel_initializer = tf.keras.initializers.glorot_uniform(seed=91), input_shape = (self.hidden_size,))
        
        return
    def get_config(self):
        cfg = super().get_config()
        return cfg    
        
    def call(self, image, question):
        
        image = self.image_dense(image)
        image = self.tanh(image)

        words = self.embed(question)   

        unigrams =  tf.expand_dims(self.tanh(self.unigram_conv(words)), 1) 
        bigrams  =  tf.expand_dims(self.tanh(self.bigram_conv(words)), 1)  
        trigrams =  tf.expand_dims(self.tanh(self.trigram_conv(words)), 1) 

        phrase = tf.squeeze(self.max_pool(tf.concat((unigrams, bigrams, trigrams), 1)), axis=1)  
        phrase = self.tanh(phrase)
        phrase = self.phrase_dropout(phrase)
  
        hidden = None
        sentence = self.lstm(phrase) 

        v_word, q_word = self.co_attention(image, words)
        v_phrase, q_phrase = self.co_attention(image, phrase)
        v_sent, q_sent = self.co_attention(image, sentence)

        h_w = self.tanh(self.W_w(self.W_w_dropout(q_word + v_word)))
        h_p = self.tanh(self.W_p(self.W_p_dropout(tf.concat(((q_phrase + v_phrase), h_w), axis=1))))
        h_s = self.tanh(self.W_s(self.W_s_dropout(tf.concat(((q_sent + v_sent), h_p), axis=1))))

        fc1 = self.fc1_Dense(self.fc1_dropout(h_s))
        logits = self.fc(fc1)

        return logits

    def co_attention(self, img_feat, ques_feat):
        img_corr = self.image_corr(img_feat)

        weight_matrix = tf.keras.backend.batch_dot(ques_feat, img_corr, axes = (2, 2))
        weight_matrix = self.tanh(weight_matrix)

        ques_embed = self.image_atten_dense(ques_feat)
        img_embed = self.question_atten_dens(img_feat)

        transform_img = tf.keras.backend.batch_dot(weight_matrix, img_embed)

        ques_atten_sum = self.tanh(transform_img + ques_embed)
        ques_atten_sum = self.question_atten_dropout(ques_atten_sum)
        ques_atten = self.ques_atten(ques_atten_sum)
        ques_atten =  tf.keras.layers.Reshape((ques_atten.shape[1],))(ques_atten)
        ques_atten =  self.softmax(ques_atten)

        # attention for image feature
        transform_ques = tf.keras.backend.batch_dot(weight_matrix, ques_embed, axes = (1, 1))
        img_atten_sum = self.tanh(transform_ques+img_embed)
        img_atten_sum = self.image_atten_dropout(img_atten_sum)
        img_atten = self.img_atten(img_atten_sum)
        img_atten = tf.keras.layers.Reshape((img_atten.shape[1],))(img_atten)
        img_atten = self.softmax(img_atten)

        ques_atten = tf.keras.layers.Reshape(( 1, ques_atten.shape[1]))(ques_atten)
        img_atten = tf.keras.layers.Reshape(( 1, img_atten.shape[1]))(img_atten)

        ques_atten_feat = tf.keras.backend.batch_dot(ques_atten,ques_feat)
        ques_atten_feat = tf.keras.layers.Reshape(( ques_atten_feat.shape[-1],))(ques_atten_feat)

        img_atten_feat =  tf.keras.backend.batch_dot(img_atten, img_feat)
        img_atten_feat = tf.keras.layers.Reshape((img_atten_feat.shape[-1],))(img_atten_feat)

        return img_atten_feat, ques_atten_feat

#Function that builds the coattention model
from keras.applications.vgg16 import VGG16
def Build_CoattentionModel(img_h, img_w, max_seq_len, num_answers, vocab_size):
    base_pretrained_model = VGG16(input_shape =  (img_h,img_w, 3), include_top = False, weights = 'imagenet')
    for layer in base_pretrained_model.layers:
      layer.trainable = False
    image_input = base_pretrained_model.input
    features_layer = base_pretrained_model.output  # (BATCH_SIZE, 512, 8, 8)
    features_layer = tf.keras.layers.Reshape((features_layer.shape[1]*features_layer.shape[2], 512), input_shape=features_layer.shape)(features_layer)
    question_input = tf.keras.layers.Input(shape=(max_seq_len,), name='Question_Input')
    #image_input = tf.keras.layers.Input(shape = (196, 512))

    output = CoattentionModel(num_classes=num_answers, num_embeddings=vocab_size)(features_layer,question_input)#num_embeddings = len(ques_vocab), num_classes = len(ans_vocab), embed_dim = 512

    model = tf.keras.models.Model(inputs = [image_input, question_input], outputs = output)

    return model

#create and compile the coattention model
model = Build_CoattentionModel(img_h, img_w, max_seq_len=max_train_lenght, num_answers=num_classes, vocab_size = len(train_wtoi)+1)
model.summary()
l_rate = 1e-3
loss = tf.keras.losses.CategoricalCrossentropy()
#loss = weighted_loss
optimizer = tf.keras.optimizers.Adam(learning_rate=l_rate)
metrics = ['accuracy']

model.compile(optimizer=optimizer, loss=loss, metrics= metrics)

"""## Optimization parameters and callbacks functions"""

#Create model callbacks: model checkpoints, tensorboard and early stopping are used
import os
from datetime import datetime

which_model = 'CustomModelVGG19VQA'

exps_dir = '/content/drive/My Drive/Homeworks/Homework3/Saved' 
if not os.path.exists(exps_dir):
    os.makedirs(exps_dir)

now = datetime.now().strftime('%b%d_%H-%M-%S')

exp_name = which_model

exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))
if not os.path.exists(exp_dir):
    os.makedirs(exp_dir)
    
callbacks = []

# Model checkpoint
ckpt_dir = os.path.join(exp_dir, 'ckpts')
if not os.path.exists(ckpt_dir):
    os.makedirs(ckpt_dir)

ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), 
                                                   save_weights_only=True,
                                                   save_best_only=True)  
callbacks.append(ckpt_callback)


# Visualize Learning on Tensorboard
tb_dir = os.path.join(exp_dir, 'tb_logs')
if not os.path.exists(tb_dir):
    os.makedirs(tb_dir)
    
tensorboardEnable = True

if tensorboardEnable:
  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,
                                               profile_batch=0,
                                               histogram_freq=1) 
  
  callbacks.append(tb_callback)

# Early Stopping
early_stop = True
if early_stop:
    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
    callbacks.append(es_callback)

#Retrieve class weights
from sklearn.utils import class_weight
y_train = train_df['answer'].to_numpy()
class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights = class_weights * 100
keys = range(0,58)
weights = dict(zip(keys, class_weights))

#Load weights

load_weights = True

loaded_weights = 'CustomModelVGG19VQA_Jan30_11-13-17/ckpts/cp_18.ckpt'

if load_weights:
  model.load_weights('/content/drive/My Drive/Homeworks/Homework3/Saved/' + str(loaded_weights))

"""## Model training."""

#Train the model
history = model.fit(x=train_dataset,
          epochs=100,
          steps_per_epoch=len(train_df)//batch_size,
          validation_data=valid_dataset,
          validation_steps=len(valid_df)//batch_size,
          callbacks = callbacks,
          #class_weight = weights,
          use_multiprocessing = True)

# plot of the training and validation loss
with plt.rc_context({'axes.edgecolor':'orange', 'xtick.color':'red', 'ytick.color':'green', 'figure.facecolor':'white'}):
    plt.plot(history.history['loss'],'r',label='training loss')
    plt.plot(history.history['val_loss'],label='validation loss')
    x=plt.xlabel('# epochs')
    y=plt.ylabel('loss')
    plt.legend()
    plt.show()

# plot of the training and validation accuracy
with plt.rc_context({'axes.edgecolor':'orange', 'xtick.color':'red', 'ytick.color':'green', 'figure.facecolor':'white'}):
    plt.plot(history.history['accuracy'],'r',label='training accuracy')
    plt.plot(history.history['val_accuracy'],label='validation accuracy')
    x=plt.xlabel('# epochs')
    y=plt.ylabel('loss')
    plt.legend()
    plt.show()

"""## Prediction and creation of the csv file.

"""

#Create the test dataset starting from the json with the same procedure of the train set
with open("/content/VQA_Dataset/test_questions.json") as read_test_file:
  test_dic = json.load(read_test_file)
test_dataframe = pd.DataFrame(test_dic.items())
test_dataframe.rename(columns = {0:'test_id', 1:'info'}, inplace = True)
L = [x for x in test_dataframe.pop('info').values]
temp_dataframe = pd.DataFrame(L)
test_dataframe = test_dataframe.join(temp_dataframe)
test_sentences = test_dataframe.get('question')
test_tokenized = train_tokenizer.texts_to_sequences(test_sentences)
max_test_lenght = max_train_lenght
test_encoder_inputs = pad_sequences(test_tokenized, maxlen = max_test_lenght, padding='post')
test_dataframe.drop('question', axis=1, inplace=True)
test_dataframe['question'] = test_encoder_inputs.tolist()
test_df = test_dataframe


test_gen = DataGenerator(img_files=test_df.get('image_id').to_list(),
                          questions=test_df.get('question').to_list(),
                          labels = [1],
                          img_dir= train_img_dir,
                          n_classes=num_classes,
                          img_data_generator=img_data_gen,
                          preprocessing_function= preprocessing,
                          dim = [img_w,img_h],
                          batch_size = 1,
                          which_subset = 'test')

test_dataset = tf.data.Dataset.from_generator(lambda: test_gen,
                                               output_types=((tf.float32, tf.int32), tf.int32),
                                               output_shapes=(([ img_h, img_w, 3],[ max_test_lenght]),(num_classes)))

test_dataset = test_dataset.batch(batch_size)

test_dataset = test_dataset.repeat()

#Function that create csv for submission
from datetime import datetime
def create_csv(results, results_dir='content/drive'):

    csv_fname = 'results_'
    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'

    with open(os.path.join(results_dir, csv_fname), 'w') as f:

        f.write('Id,Category\n')

        for key, value in results:
            f.write(key + ',' + str(value) + '\n')

#Predictions and csv creation
predictions = model.predict(x=test_dataset, batch_size=batch_size, verbose=1, steps=(len(test_dataframe)//batch_size)+1 ) 
results = []
for i in range(0, len(test_dataframe)):
  results.append((test_dataframe.test_id[i], np.argmax(predictions[i])))

create_csv(results, '/content/drive/My Drive')
